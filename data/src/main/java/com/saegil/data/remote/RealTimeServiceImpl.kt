package com.saegil.data.remote

import android.media.AudioFormat
import android.media.AudioManager
import android.media.AudioRecord
import android.media.AudioTrack
import android.media.MediaRecorder
import android.os.Build
import androidx.annotation.RequiresApi
import com.saegil.data.BuildConfig
import io.ktor.client.HttpClient
import io.ktor.client.plugins.websocket.DefaultClientWebSocketSession
import io.ktor.client.plugins.websocket.webSocket
import io.ktor.client.request.header
import io.ktor.client.request.url
import io.ktor.http.HttpHeaders
import io.ktor.http.HttpMethod
import io.ktor.websocket.Frame
import io.ktor.websocket.close
import io.ktor.websocket.readText
import kotlinx.coroutines.CoroutineScope
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.launch
import kotlinx.serialization.json.Json
import kotlinx.serialization.json.jsonObject
import kotlinx.serialization.json.jsonPrimitive
import java.util.Base64

class RealTimeServiceImpl(
    private val client: HttpClient
) : RealTimeService {

    private var session: DefaultClientWebSocketSession? = null
    private var isStreaming = false

    @RequiresApi(Build.VERSION_CODES.O)
    override suspend fun connectToRealtimeSession(secret: String) {
        try {
            client.webSocket(
                method = HttpMethod.Get,
                host = "api.openai.com",
                path = "/v1/realtime/sessions/$secret",
                request = {
                    url("wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17&modalities=audio")
//                    header(HttpHeaders.Authorization, "Bearer ${BuildConfig.OPEN_AI_API_KEY}")
                    header(HttpHeaders.Authorization, "Bearer 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")
                    header("OpenAI-Beta", "realtime=v1")
                }
            ) {
                println("âœ… WebSocket connected")
                session = this
                isStreaming = true

                launch(Dispatchers.IO) {
                    startAudioStreaming(this@webSocket)
                }

                for (frame in incoming) {
                    if (frame is Frame.Text) {
                         val json = frame.readText()
                        println("ğŸ“¨ Received: $json")

                        val jsonObj = Json.parseToJsonElement(json).jsonObject
                        when (jsonObj["type"]?.jsonPrimitive?.content) {
                            "session.created" -> {
                                // Handle session creation confirmation.
                                // You might want to store the session ID here.
                                val sessionId = jsonObj["session"]?.jsonObject?.get("id")?.jsonPrimitive?.content
                                println("Session created with ID: $sessionId")
                            }
                            // This is the crucial part: handling the audio delta messages
                            "response.audio.delta" -> {
                                // Extract the "delta" field which contains the Base64 encoded audio bytes
                                val base64Audio = jsonObj["delta"]?.jsonPrimitive?.content ?: ""
                                if (base64Audio.isNotEmpty()) {
                                    try {
                                        // Decode the Base64 string to a ByteArray
                                        val audioBytes = Base64.getDecoder().decode(base64Audio)
                                        // Write the audio bytes to the AudioTrack for playback
                                        playAudio(audioBytes)
                                    } catch (e: IllegalArgumentException) {
                                        println("âŒ Base64 decoding error: ${e.message}")
                                    }
                                }
                            }
                            "response.completed" -> {
                                // The AI's response has completed.
                                // You might want to signal the end of speech here,
                                // but keep the AudioTrack playing until its buffer is empty.
                                println("AI response completed.")
                            }
                            // Handle other message types if necessary (e.g., input_audio_buffer.speech_started/stopped)
                            else -> {
                                // println("Received message of type: ${jsonObj["type"]?.jsonPrimitive?.content}")
                            }
//                            "audio" -> {
//                                val base64Audio = jsonObj["data"]?.jsonPrimitive?.content ?: ""
//                                val audioBytes = Base64.getDecoder().decode(base64Audio)
//                                playAudio(audioBytes)
//                            }
//                            else -> {
//                                // ë‹¤ë¥¸ ë©”ì‹œì§€ íƒ€ì… ì²˜ë¦¬ (ì˜ˆ: session.created, error ë“±)
////                                println("Received message but not audio: $json")
//                            }
                        }
                    }
                }
            }
        } catch (e: Exception) {
            println("âŒ WebSocket error: ${e.message}")
        } finally {
            isStreaming = false
        }
    }

    @RequiresApi(Build.VERSION_CODES.O)
    override suspend fun sendPcm(pcm: ByteArray) {
        session?.let {
            RealtimeMessageSender.sendPcmAudio(it, pcm)
        } ?: println("âŒ No active WebSocket session to send PCM")
    }

    override suspend fun commitAudio() {
        session?.let {
            RealtimeMessageSender.commitAudio(it)
        } ?: println("âŒ No active WebSocket session to commit audio")
    }

    @RequiresApi(Build.VERSION_CODES.O)
    private fun startAudioStreaming(ws: DefaultClientWebSocketSession) {
        val recorder = AudioRecord(
            MediaRecorder.AudioSource.MIC,
            SAMPLE_RATE,
            CHANNEL_CONFIG,
            AUDIO_FORMAT,
            BUFFER_SIZE
        )

        if (recorder.state != AudioRecord.STATE_INITIALIZED) {
            println("âŒ AudioRecord ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
        }

        val buffer = ByteArray(BUFFER_SIZE)
        var appendCount = 0

        recorder.startRecording()
        println("ğŸ™ï¸ Audio recording started")

        while (isStreaming) {
            val read = recorder.read(buffer, 0, buffer.size)
            if (read > 0) {
                val audioChunk = buffer.copyOf(read)

                CoroutineScope(Dispatchers.IO).launch {
                    RealtimeMessageSender.sendPcmAudio(ws, audioChunk)
                }

                appendCount++
            } else {
                println("âš ï¸ Audio read failed or empty (read=$read)")
            }

            if (appendCount >= 10) {
                CoroutineScope(Dispatchers.IO).launch {
                    RealtimeMessageSender.commitAudio(ws)
                }
                appendCount = 0
            }
        }

        recorder.stop()
        recorder.release()

        CoroutineScope(Dispatchers.IO).launch {
            RealtimeMessageSender.commitAudio(ws)
        }

        println("ğŸ™ï¸ Audio recording stopped and committed")
    }

    private var audioTrack: AudioTrack? = null
    private var isAudioPlaying = false
    private val audioQueue: ArrayDeque<ByteArray> = ArrayDeque()

    private fun playAudio(audio: ByteArray) {
        // ì¬ìƒ ì¤‘ì´ë©´ íì— ì¶”ê°€ë§Œ
        if (isAudioPlaying) {
            audioQueue.addLast(audio)
            println("ğŸ“¥ Queued audio chunk. Queue size: ${audioQueue.size}")
            return
        }

        // ë‹¤ìŒ ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘
        audioTrack?.release()

        val track = AudioTrack(
            AudioManager.STREAM_MUSIC,
            SAMPLE_RATE,
            AudioFormat.CHANNEL_OUT_MONO,
            AudioFormat.ENCODING_PCM_16BIT,
            audio.size,
            AudioTrack.MODE_STATIC,
            AudioManager.AUDIO_SESSION_ID_GENERATE
        )

        track.setNotificationMarkerPosition(audio.size / 2)
        track.setPlaybackPositionUpdateListener(object : AudioTrack.OnPlaybackPositionUpdateListener {
            override fun onMarkerReached(track: AudioTrack?) {
                isAudioPlaying = false
                println("âœ… Finished audio chunk")

                // ë‹¤ìŒ ì˜¤ë””ì˜¤ê°€ ìˆë‹¤ë©´ ì¬ìƒ
                if (audioQueue.isNotEmpty()) {
                    val nextAudio = audioQueue.removeFirst()
                    playAudio(nextAudio)
                }
            }

            override fun onPeriodicNotification(track: AudioTrack?) {
                // Not used
            }
        })

        audioTrack = track
        isAudioPlaying = true

        track.write(audio, 0, audio.size)
        track.play()

        println("ğŸ”Š Playing audio chunk. Queue size: ${audioQueue.size}")
    }


    companion object{
        private const val SAMPLE_RATE = 16000 // 16kHz
        private const val CHANNEL_CONFIG = AudioFormat.CHANNEL_IN_MONO // ëª¨ë…¸ ì±„ë„
        private const val AUDIO_FORMAT = AudioFormat.ENCODING_PCM_16BIT // 16bit signed PCM
        private val BUFFER_SIZE = AudioRecord.getMinBufferSize(
            SAMPLE_RATE,
            CHANNEL_CONFIG,
            AUDIO_FORMAT
        ).coerceAtLeast(SAMPLE_RATE * 2) // 1ì´ˆ ë¶„ëŸ‰ or ìµœì†Œê°’ ì´ìƒ

    }

    override suspend fun disconnect() {
        isStreaming = false
        session?.close()
        session = null
        println("ğŸ”Œ WebSocket disconnected")
    }

}
